{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adult-secondary",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://vbti.nl\"><img src=\"https://docs.google.com/uc?export=download&id=1DdCGllL51O5wBuiI0rwygofKx3YIDPHX\" width=\"400\"></a>\n",
    "</div>\n",
    "\n",
    "# Contextualized Embeddings with a Transformer\n",
    "\n",
    "In the previous notebooks, we have used the Word2Vec model to learn the embeddings from the text. The embeddings are learned in the unsupervised setting, by predicting the word given the context (or the context given a word). While these embeddings are quite useful, they have one major downside: once learned, they are separated from the context. A single word can mean different things when surrounded by different words. For example, the word \"transformer\" could refer to an electrical device, Optimus Prime, or neural network architecture. It is impossible to determine the meaning of the word without its context. The idea behind ELMo presented in [original paper](https://arxiv.org/pdf/1802.05365.pdf) utilizes this idea and forms **contextualized embeddings** for the word taking into account its neighbors. It does so by employing bi-directional LSTM, which looks at words that came before and after to form the embeddings. As we have already learned recurrent neural networks are very hard to train due to the backpropagation through time. This brings to major downsides of the RNN: 1) not being able to utilize parallel computations and 2) difficulty with retaining long-term dependencies. \n",
    "\n",
    "With the release of the Transformer paper, called [Attention is All You Need](https://arxiv.org/abs/1706.03762), this architecture started to dominate the NLP world. The Transformer architecture is parallelizable, which allows the utilization of multiple GPUs and TPUs, and also it can model long-term relationships with the novel attention mechanism. Unfortunately, we are not able to dive deep into the topics of transformers, since they deserve their own Masterclass (or two).\n",
    "\n",
    "If you want to understand better how the transformer works or why is it so good for modeling long-term dependencies, check a visual description of the transformers here: https://jalammar.github.io/illustrated-transformer/.\n",
    "\n",
    "We are going to use a specific transformer architecture called Bidirectional Encoder Representations from Transformers (BERT). The original paper from Google Research can be found here: https://arxiv.org/pdf/1810.04805.pdf. For a more informal and visual guide check out Illustrated BERT: http://jalammar.github.io/illustrated-bert/.\n",
    "\n",
    "In this notebook, we are going to have a look at how we can use pre-trained BERT for a custom classification task. We are going to use [`transformers`](https://pypi.org/project/transformers/) package from [HuggingFace](https://huggingface.co/transformers/index.html). There we can find a great amount of pre-trained models that are easily reusable and fine-tunable to almost any NLP task. Thanks to the Abhishek Thakur and his [book](https://www.amazon.com/Approaching-Almost-Machine-Learning-Problem-ebook/dp/B089P13QHT) for the inspiration.\n",
    "\n",
    "Learning goals:\n",
    " - Learn how to use a pre-trained tokenizer\n",
    " - Learn how to prepare the dataset for BERT\n",
    " - Learn how to fine-tune BERT for a classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-river",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seeing-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-removal",
   "metadata": {},
   "source": [
    "## Defining Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conscious-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# this is the maximum number of tokens in the sentence\n",
    "MAX_LEN = 512\n",
    "# batch sizes is small because model is huge!\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "# let's train for a maximum of 10 epochs\n",
    "EPOCHS = 10\n",
    "# define path to BERT model files\n",
    "BERT_PATH = \"bert-base-uncased\"\n",
    "# this is where you want to save the model\n",
    "MODEL_PATH = \"model.bin\"\n",
    "# training file\n",
    "TRAINING_FILE = \"https://raw.githubusercontent.com/illyakaynov/masterclass-nlp/master/Case-IMBD_reviews/IMDB.csv\"\n",
    "# define the tokenizer\n",
    "# we use tokenizer and model\n",
    "# from huggingface's transformers\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    " BERT_PATH,\n",
    " do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-probe",
   "metadata": {},
   "source": [
    "One of the things that we have defined here is the `BERT_PATH`. This is the name of the model that we are going to use. HuggingFace has implemented dozens of models, which were pre-trained on a big corpus of data. Check out the available models [here](https://huggingface.co/transformers/pretrained_models.html). The `base` part means this is a smaller model with 12 transformer blocks, 12 attention heads, and 768 dimensions for the Feed Forward Network layer. The `large` version has 24 transformer blocks with 16 attention heads and 1024 hidden units for FNN.\n",
    "\n",
    "We also need to define the `TOKENIZER` which will transform the words in a sentence from strings into integers. We would need to use the same tokenizer that was used during BERT's training, otherwise, the mapping from words to integers will be different which will cause problems. The tokenizer has a nice method `encode_plus()` which will take our reviews and return the encoded version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "differential-adams",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2023, 2003, 2019, 2742, 1997, 1037, 6251, 2000, 19204, 4697, 102]\n"
     ]
    }
   ],
   "source": [
    "example_sentence = 'This is an example of a sentence to tokenize'\n",
    "tokenized_sentence = TOKENIZER.encode_plus(\n",
    "    example_sentence,\n",
    "    None,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=15,\n",
    ")\n",
    "print(tokenized_sentence['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-palestine",
   "metadata": {},
   "source": [
    "We can map the tokens back to a sentence with `convert_ids_to_tokens()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "improved-puzzle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'an', 'example', 'of', 'a', 'sentence', 'to', 'token', '##ize', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(TOKENIZER.convert_ids_to_tokens(tokenized_sentence['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-validation",
   "metadata": {},
   "source": [
    "The attribute `pad_to_max_length=True`, `max_length=15` made sure that the sequences are padded with zeros so that they match specified length. You can leave `max_length=None` so that they match the maximum length used for model training, in the case of BERT is 512.\n",
    "\n",
    "You can also notice that there are special symbols added:\n",
    "- [CLS] - stands for classification\n",
    "- [SEP] - indicates the end of the sentence\n",
    "- [PAD] - padding values\n",
    "\n",
    "There are more values that are returned by a tokenizer in a dictionary. Lets check all the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranking-warehouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-shark",
   "metadata": {},
   "source": [
    "`token_type_ids` is the remnant of two sentence tasks. In this case, zeros will be set for tokens from the first sentence and ones for the second. `attention_mask` will indicate which values the model needs to take into account when calculating attention, i.e. ignoring the padded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complete-stone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'of', 'a', 'sentence', 'to', 'tokenize']\n",
      "[101, 2023, 2003, 2019, 2742, 1997, 1037, 6251, 2000, 19204, 4697, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "['[CLS]', 'this', 'is', 'an', 'example', 'of', 'a', 'sentence', 'to', 'token', '##ize', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(example_sentence.split())\n",
    "print(tokenized_sentence['input_ids'])\n",
    "print(tokenized_sentence['token_type_ids'])\n",
    "print(tokenized_sentence['attention_mask'])\n",
    "print(TOKENIZER.convert_ids_to_tokens(tokenized_sentence['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-dragon",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "We are going to use IMDB movie review dataset. `BERTDataset` class will be responsible for preprocessing the reviews so that they can be used as input into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "protecting-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class BERTDataset:\n",
    "    def __init__(self, review, target):\n",
    "        \"\"\"\n",
    "        :param review: list or numpy array of strings\n",
    "        :param targets: list or numpy array which is binary\n",
    "        \"\"\"\n",
    "        self.review = review\n",
    "        self.target = target\n",
    "        \n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "        \n",
    "    def __len__(self):\n",
    "        # this returns the length of dataset\n",
    "        return len(self.review)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # for a given item index, return a dictionary\n",
    "        # of inputs\n",
    "        review = str(self.review[item])\n",
    "        review = \" \".join(review.split())\n",
    "        # encode_plus comes from hugginface's transformers\n",
    "        # and exists for all tokenizers they offer\n",
    "        # it can be used to convert a given string\n",
    "        # to ids, mask and token type ids which are\n",
    "        # needed for models like BERT\n",
    "        # here, review is a string\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        # ids are ids of tokens generated\n",
    "        # after tokenizing reviews\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        # mask is 1 where we have input\n",
    "        # and 0 where we have padding\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        # token type ids behave the same way as\n",
    "        # mask in this specific case\n",
    "        # in case of two sentences, this is 0\n",
    "        # for first sentence and 1 for second sentence\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        # now we return everything\n",
    "        # note that ids, mask and token_type_ids\n",
    "        # are all long datatypes and targets is float\n",
    "        return {\n",
    "            \"ids\": torch.tensor(\n",
    "                ids, dtype=torch.long\n",
    "            ),\n",
    "            \"mask\": torch.tensor(\n",
    "                mask, dtype=torch.long\n",
    "            ),\n",
    "            \"token_type_ids\": torch.tensor(\n",
    "                token_type_ids, dtype=torch.long\n",
    "            ),\n",
    "            \"targets\": torch.tensor(\n",
    "                self.target[item], dtype=torch.float\n",
    "            )\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-willow",
   "metadata": {},
   "source": [
    "Lets have a look at the example dataset. We are going to use the familiar `torch.utils.data.DataLoader` to form batches for training and validation. But first lets turn encode the labels in classes as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intimate-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAINING_FILE)\n",
    "# encode labels\n",
    "df.sentiment = df.sentiment.apply(\n",
    "    lambda x: 1 if x == \"positive\" else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cooked-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      One of the other reviewers has mentioned that ...          1\n",
       "1      A wonderful little production. <br /><br />The...          1\n",
       "2      I thought this was a wonderful way to spend ti...          1\n",
       "3      Basically there's a family where a little boy ...          0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "...                                                  ...        ...\n",
       "49995  I thought this movie did a down right good job...          1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
       "49997  I am a Catholic taught in parochial elementary...          0\n",
       "49998  I'm going to have to disagree with the previou...          0\n",
       "49999  No one expects the Star Trek movies to be high...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "better-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = BERTDataset(df.review.values, df.sentiment.values)\n",
    "example_data_loader = torch.utils.data.DataLoader(\n",
    "    example_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "sample = next(iter(example_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "narrow-importance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([[  101,  2028,  1997,  ...,     0,     0,     0],\n",
       "         [  101,  1037,  6919,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2245,  ...,     0,     0,     0],\n",
       "         [  101, 10468,  2045,  ...,     0,     0,     0]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([1., 1., 1., 0.])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-hypothesis",
   "metadata": {},
   "source": [
    "## Pre-trained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wicked-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.BertModel.from_pretrained(BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "appropriate-personal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "serious-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state, pool = model(\n",
    "    sample['ids'],\n",
    "    attention_mask=sample['mask'],\n",
    "    token_type_ids=sample['token_type_ids'],\n",
    "    return_dict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-season",
   "metadata": {},
   "source": [
    "Lets have a look at the shapes. The last hidden state has the the shape (batch_size, seq_length, n_hidden). While the `pooler` has shape of (batch_size, n_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "automatic-strap",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "binary-salem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-requirement",
   "metadata": {},
   "source": [
    "By default, BERT returns two outputs: the last hidden state and the output of the pooler layer. The pooled output is produced by processing all contextual embeddings in a sequence with a Feed-Forward Network. If the last hidden state contains all **contextual embeddings** for each word in a sequence, then the pooler layer is an embedding of a document, or in this case the review. The only thing that is left to do is to train an additional Dense layer to separate these documents into the two categories. You can check out a nice visualization of all embeddings formed by BERT in this [article](https://towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dominant-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the model from the memory\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-burden",
   "metadata": {},
   "source": [
    "## Building a Model for Classification\n",
    "Now we will take the pre-trained BERT model and encapsulate it in a class. As discussed we will add a single `Linear` layer at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "victorian-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
    "        # add a dropout for regularization\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        # a simple linear layer for output\n",
    "        # yes, there is only one output\n",
    "        self.out = nn.Linear(768, 1)\n",
    "    def forward(self, ids, mask, token_type_ids, return_dict=False):\n",
    "        # BERT in its default settings returns two outputs\n",
    "        # last hidden state and output of bert pooler layer\n",
    "        # we use the output of the pooler which is of the size\n",
    "        # (batch_size, hidden_size)\n",
    "        # hidden size can be 768 or 1024 depending on\n",
    "        # if we are using bert base or large respectively\n",
    "        # in our case, it is 768\n",
    "        # note that this model is pretty simple\n",
    "        # you might want to use last hidden state\n",
    "        # or several hidden states\n",
    "        _, o2 = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "        # pass through dropout layer\n",
    "        bo = self.bert_drop(o2)\n",
    "        # pass through linear layer\n",
    "        output = self.out(bo)\n",
    "        # return output\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-broadway",
   "metadata": {},
   "source": [
    "We are also going to define a loss function and the training method. For the loss, we are going to use binary cross entropy with logits, since we training the model for binary classification. The training method will look very similar to what you have used already. Check out the comments in the code for precise steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "appreciated-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "def loss_fn(outputs, targets):\n",
    "    \"\"\"\n",
    "    This function returns the loss.\n",
    "    :param outputs: output from the model (real numbers)\n",
    "    :param targets: input targets (binary)\n",
    "    \"\"\"\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    \"\"\"\n",
    "    This is the training function which trains for one epoch\n",
    "    :param data_loader: it is the torch dataloader object\n",
    "    :param model: torch model, bert in our case\n",
    "    :param optimizer: adam, sgd, etc\n",
    "    :param device: can be cpu or cuda\n",
    "    :param scheduler: learning rate scheduler\n",
    "    \"\"\"\n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "    # loop over all batches\n",
    "    for d in tqdm(data_loader):\n",
    "        # extract ids, token type ids and mask\n",
    "        # from current batch\n",
    "        # also extract targets\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets = d[\"targets\"]\n",
    "        # move everything to specified device\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        # zero-grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # pass through the model\n",
    "        outputs = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=False\n",
    "\n",
    "        )\n",
    "        # calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        # backward step the loss\n",
    "        loss.backward()\n",
    "        # step optimizer\n",
    "        optimizer.step()\n",
    "        # step scheduler\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "korean-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    this is the validation function that generates\n",
    "    predictions on validation data\n",
    "    :param data_loader: it is the torch dataloader object\n",
    "    :param model: torch model, bert in our case\n",
    "    :param device: can be cpu or cuda\n",
    "    :return: output and targets\n",
    "    \"\"\"\n",
    "    # put model in eval mode\n",
    "    model.eval()\n",
    "    # initialize empty lists for\n",
    "    # targets and outputs\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    # use the no_grad scope\n",
    "    # its very important else you might\n",
    "    # run out of gpu memory\n",
    "    with torch.no_grad():\n",
    "        # this part is same as training function\n",
    "        # except for the fact that there is no\n",
    "        # zero_grad of optimizer and there is no loss\n",
    "        # calculation or scheduler steps.\n",
    "        for d in tqdm(data_loader):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets = d[\"targets\"]\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            outputs = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                return_dict=False\n",
    "            )\n",
    "            # convert targets to cpu and extend the final list\n",
    "            targets = targets.cpu().detach()\n",
    "            fin_targets.extend(targets.numpy().tolist())\n",
    "            # convert outputs to cpu and extend the final list\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach()\n",
    "            fin_outputs.extend(outputs.numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "inside-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, valid_data_loader, n_epochs=1, lr=3e-5, device='cuda'):\n",
    "    # this function trains the model\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    # create parameters we want to optimize\n",
    "    # we generally dont use any decay for bias\n",
    "    # and weight layers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if\n",
    "                not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if\n",
    "                any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    # calculate the number of training steps\n",
    "    # this is used by scheduler\n",
    "    num_train_steps = int(\n",
    "        len(df_train) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "    )\n",
    "    # AdamW optimizer\n",
    "    # AdamW is the most widely used optimizer\n",
    "    # for transformer based networks\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    # fetch a scheduler\n",
    "    # you can also try using reduce lr on plateau\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "    # if you have multiple GPUs\n",
    "    # model model to DataParallel\n",
    "    # to use multiple GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "    # start training the epochs\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_fn(\n",
    "            train_data_loader, model, optimizer, device, scheduler\n",
    "        )\n",
    "        outputs, targets = eval_fn(\n",
    "            valid_data_loader, model, device\n",
    "        )\n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        print(f\"Accuracy Score = {accuracy}\")\n",
    "        if accuracy > best_accuracy:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "technical-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training file and fill NaN values with \"none\"\n",
    "# you can also choose to drop NaN values\n",
    "dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
    "# sentiment = 1 if its positive\n",
    "# else sentiment = 0\n",
    "dfx.sentiment = dfx.sentiment.apply(\n",
    "    lambda x: 1 if x == \"positive\" else 0\n",
    ")\n",
    "# we split the data into single training\n",
    "# and validation fold\n",
    "df_train, df_valid = model_selection.train_test_split(\n",
    "    dfx,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=dfx.sentiment.values\n",
    ")\n",
    "# reset index\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "# for training dataset\n",
    "train_dataset = BERTDataset(\n",
    "    review=df_train.review.values,\n",
    "    target=df_train.sentiment.values\n",
    ")\n",
    "# create training dataloader\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# for validation dataset\n",
    "valid_dataset = BERTDataset(\n",
    "    review=df_valid.review.values,\n",
    "    target=df_valid.sentiment.values\n",
    ")\n",
    "# create validation data loader\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERTBaseUncased()\n",
    "train(bert_model, train_data_loader, valid_data_loader, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-crest",
   "metadata": {},
   "source": [
    "The training will take approximately 40 min per epoch. Training even for one epoch will give a nice result.\n",
    "\n",
    "We have also included pre-trained weights. That you can use. Next cell will download the weights for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-lottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"model.bin\" already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "def download_file(url, path):\n",
    "    \"\"\"\n",
    "    Download file and save it to the defined location\n",
    "    \n",
    "    https://stackoverflow.com/questions/37573483/progress-bar-while-download-file-over-http-with-requests/37573701\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from tqdm.notebook import tqdm\n",
    "    import os\n",
    "    \n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        print('File \"{}\" already exists. Skipping download.'.format(path))\n",
    "        return\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 #1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(path, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "        \n",
    "download_file(\n",
    "    'https://github.com/illyakaynov/masterclass_datasets/raw/master/RNN%20and%20Transformers/BERT_IMDB.bin',\n",
    "    'model.bin'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aboriginal-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = BERTBaseUncased()\n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "bert_model = load_model('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "worst-thong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63a19f1890b4dbf9a51a99f395cf536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.947\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = eval_fn(\n",
    "            valid_data_loader, bert_model, 'cuda'\n",
    "        )\n",
    "outputs = np.array(outputs) >= 0.5\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "print(f\"Accuracy Score = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-planning",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Use different model\n",
    "- Adjust the implementation for a multi-class classification problem\n",
    "- Use the model for different task, for example: question answering, or machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-rebecca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
