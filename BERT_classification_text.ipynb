{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "developing-missouri",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://vbti.nl\"><img src=\"https://docs.google.com/uc?export=download&id=1DdCGllL51O5wBuiI0rwygofKx3YIDPHX\" width=\"400\"></a>\n",
    "</div>\n",
    "\n",
    "# Text Classification with Transformers\n",
    "\n",
    "In the previus notebooks we have used the Word2Vec model to learn the embeddings from text. The embeddings are learned in the unsupervised setting, by predicting the next word in the sequence. While these embeddigs are quite usefull, they have one major downside: once learned, they are separated from the context. A single word can mean different things when placed in different context. For example the word \"transformer\" could refer to electrical device, Optimus Prime or to neural network architecture. It is impossible to determine the meanin of the word without its context. The idea behind ELMo presented in [original paper](https://arxiv.org/pdf/1802.05365.pdf) utilizes this idea and forms embeddings for the word taking into the acount its neighoughrs. It does so by employing bi-directional LSTM, that looks at words that came before and after to form the embeddings. As we have already learned recurrent neaural networks are very hard to train due to the backpropagation to time. This brings to major downsides of the RNN: 1) not being able to utilize parallel computations and 2) difficulty with retaining long term dependancies. \n",
    "\n",
    "With the release of the Transformer paper, called [Attention is All You Need](https://arxiv.org/abs/1706.03762), this architecture started to dominate NLP world. The Transformer architecture is parallelizable, which allows to utilize the power of multiple GPUs and TPUs, and also it is able to model long-term relationships with the attention mechanism.\n",
    "\n",
    "For visual description of the transformers see: https://jalammar.github.io/illustrated-transformer/. For BERT: http://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "\n",
    "In this notebook we are going to have a look how we can use pre-trained BERT for a custom classification task. We are going to use [`transformers`](https://pypi.org/project/transformers/) package from [HuggingFace](https://huggingface.co/transformers/index.html). There we can find a great amount of pre-trained models that are easily reusable and fine-tunable to almost any NLP task.\n",
    "\n",
    "Learning goals:\n",
    " - Learn how to use pre-trained tokenizer\n",
    " - Learn how to prepare the dataset for BERT\n",
    " - Learn how to fine-tune BERT for a classification task\n",
    " \n",
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "revolutionary-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-radar",
   "metadata": {},
   "source": [
    "## Defining Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "common-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# this is the maximum number of tokens in the sentence\n",
    "MAX_LEN = 512\n",
    "# batch sizes is small because model is huge!\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "# let's train for a maximum of 10 epochs\n",
    "EPOCHS = 10\n",
    "# define path to BERT model files\n",
    "BERT_PATH = \"bert-base-uncased\"\n",
    "# this is where you want to save the model\n",
    "MODEL_PATH = \"model.bin\"\n",
    "# training file\n",
    "TRAINING_FILE = \"https://raw.githubusercontent.com/illyakaynov/masterclass-nlp/master/Case-IMBD_reviews/IMDB.csv\"\n",
    "# define the tokenizer\n",
    "# we use tokenizer and model\n",
    "# from huggingface's transformers\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    " BERT_PATH,\n",
    " do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-committee",
   "metadata": {},
   "source": [
    "One of the things that we have defined here is the `BERT_PATH`. This is the name of the model that we are going to use. The `base` part means this is a smaller model with 12 transformer blocks, 12 attention heads and 768 dimensions for the Feed Forward Network layer. The `large` version has 24 transformer blocks with 16 attention heads and 1024 hidden units for FNN.\n",
    "\n",
    "We also need to defined the `TOKENIZER` which will transform the words in a sentences from strings into integers. We would need to use the same tokenizer that was used during BERT's training, otherwise the mapping will from words to integers will be different wich will cause problems. The tokenizer has a nice method `encode_plus()` which will take our reviews and return the encoded version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "artistic-luther",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2023, 2003, 2019, 2742, 1997, 1037, 6251, 2000, 19204, 4697, 102, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "example_sentence = 'This is an example of a sentence to tokenize'\n",
    "tokenized_sentence = TOKENIZER.encode_plus(\n",
    "    example_sentence,\n",
    "    None,\n",
    "    add_special_tokens=True,\n",
    "    max_length=15,\n",
    "    pad_to_max_length=True,\n",
    ")\n",
    "print(tokenized_sentence['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-analyst",
   "metadata": {},
   "source": [
    "We can map the tokens back to a sentence with `convert_ids_to_tokens()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cosmetic-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'an', 'example', 'of', 'a', 'sentence', 'to', 'token', '##ize', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(TOKENIZER.convert_ids_to_tokens(tokenized_sentence['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-picnic",
   "metadata": {},
   "source": [
    "The attribute `pad_to_max_length=True`, `max_length=15` made sure that the sequences are padded with zeros so that they match specified length. You can leave `max_length=None` so that they match the maximum length used for model training, in case of BERT is 512.\n",
    "\n",
    "You can also notice that there are special symbols added:\n",
    "- [CLS] - stands for classification\n",
    "- [SEP] - indicates the end of the sentence\n",
    "- [PAD] - padding values\n",
    "\n",
    "There are more values that are returned by a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "humanitarian-balloon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-exclusive",
   "metadata": {},
   "source": [
    "`token_type_ids` is the remnant of two sentence tasks. In this case zeros will be set for tokens from the first sentence and ones for the second. `attention_mask` will indicate which values the model need to take into the account when calculating attention, i.e. ignoring the padded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "promotional-chase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'of', 'a', 'sentence', 'to', 'tokenize']\n",
      "[101, 2023, 2003, 2019, 2742, 1997, 1037, 6251, 2000, 19204, 4697, 102, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "['[CLS]', 'this', 'is', 'an', 'example', 'of', 'a', 'sentence', 'to', 'token', '##ize', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(example_sentence.split())\n",
    "print(tokenized_sentence['input_ids'])\n",
    "print(tokenized_sentence['token_type_ids'])\n",
    "print(tokenized_sentence['attention_mask'])\n",
    "print(TOKENIZER.convert_ids_to_tokens(tokenized_sentence['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-forty",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "We are going to use IMDB movie review dataset. `BERTDataset` class will be responsible for preprocessing the reviews so that they can be used as input into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fundamental-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class BERTDataset:\n",
    "    def __init__(self, review, target):\n",
    "        \"\"\"\n",
    "        :param review: list or numpy array of strings\n",
    "        :param targets: list or numpy array which is binary\n",
    "        \"\"\"\n",
    "        self.review = review\n",
    "        self.target = target\n",
    "        \n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "        \n",
    "    def __len__(self):\n",
    "        # this returns the length of dataset\n",
    "        return len(self.review)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # for a given item index, return a dictionary\n",
    "        # of inputs\n",
    "        review = str(self.review[item])\n",
    "        review = \" \".join(review.split())\n",
    "        # encode_plus comes from hugginface's transformers\n",
    "        # and exists for all tokenizers they offer\n",
    "        # it can be used to convert a given string\n",
    "        # to ids, mask and token type ids which are\n",
    "        # needed for models like BERT\n",
    "        # here, review is a string\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,)\n",
    "        # ids are ids of tokens generated\n",
    "        # after tokenizing reviews\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        # mask is 1 where we have input\n",
    "        # and 0 where we have padding\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        # token type ids behave the same way as\n",
    "        # mask in this specific case\n",
    "        # in case of two sentences, this is 0\n",
    "        # for first sentence and 1 for second sentence\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        # now we return everything\n",
    "        # note that ids, mask and token_type_ids\n",
    "        # are all long datatypes and targets is float\n",
    "        return {\n",
    "            \"ids\": torch.tensor(\n",
    "                ids, dtype=torch.long\n",
    "            ),\n",
    "            \"mask\": torch.tensor(\n",
    "                mask, dtype=torch.long\n",
    "            ),\n",
    "            \"token_type_ids\": torch.tensor(\n",
    "                token_type_ids, dtype=torch.long\n",
    "            ),\n",
    "            \"targets\": torch.tensor(\n",
    "                self.target[item], dtype=torch.float\n",
    "            )\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-cooper",
   "metadata": {},
   "source": [
    "Lets have a look at the example dataset. We are going to use the familiar `torch.utils.data.DataLoader` to form batches for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "signal-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAINING_FILE)\n",
    "# encode labels\n",
    "df.sentiment = df.sentiment.apply(\n",
    "    lambda x: 1 if x == \"positive\" else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "mechanical-undergraduate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      One of the other reviewers has mentioned that ...          1\n",
       "1      A wonderful little production. <br /><br />The...          1\n",
       "2      I thought this was a wonderful way to spend ti...          1\n",
       "3      Basically there's a family where a little boy ...          0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "...                                                  ...        ...\n",
       "49995  I thought this movie did a down right good job...          1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
       "49997  I am a Catholic taught in parochial elementary...          0\n",
       "49998  I'm going to have to disagree with the previou...          0\n",
       "49999  No one expects the Star Trek movies to be high...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "desperate-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = BERTDataset(df.review.values, df.sentiment.values)\n",
    "example_data_loader = torch.utils.data.DataLoader(\n",
    "    example_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "sample = next(iter(example_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "resident-andorra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([[  101,  2028,  1997,  ...,     0,     0,     0],\n",
       "         [  101,  1037,  6919,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2245,  ...,     0,     0,     0],\n",
       "         [  101, 10468,  2045,  ...,     0,     0,     0]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([1., 1., 1., 0.])}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-press",
   "metadata": {},
   "source": [
    "## Pre-trained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.BertModel.from_pretrained(BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "illegal-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "novel-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state, pool = model(\n",
    "    sample['ids'],\n",
    "    attention_mask=sample['mask'],\n",
    "    token_type_ids=sample['token_type_ids'],\n",
    "    return_dict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-suggestion",
   "metadata": {},
   "source": [
    "Lets have a look at the shapes. The last hidden state has the the shape (batch_size, seq_length, n_hidden). While the `pooler` has shape of (batch_size, n_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "experienced-richardson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 768])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "engaging-heading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-venice",
   "metadata": {},
   "source": [
    "By default BERT returns two outputs: last hidden state and the output of the pooler layer. The pooled output is produced by processing all contextual embeddings in a sequence with a Feed Forward Network. If the last hidden state contains all **contextual embeddings** for each word in a sequence, then the pooler layer is an embedding of a document, or in this case the review. The only thing that is left to do is to train an additional Dense layer to separate these documents into the two categories. You can check out a nice visualization of all embeddings formed by BERT in this [article](https://towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "usual-seventh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522, 768])\n",
      "torch.Size([512, 768])\n",
      "torch.Size([2, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-estate",
   "metadata": {},
   "source": [
    "## Building a Model for Classification\n",
    "Now we will take the pre-trained BERT model and encapsulate it in a class. As discussed we will add a single `Linear` layer at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dominant-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self, n_classes=1):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
    "        # add a dropout for regularization\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        # a simple linear layer for output\n",
    "  \n",
    "        self.out = nn.Linear(768, n_classes)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        # BERT in its default settings returns two outputs\n",
    "        # last hidden state and output of bert pooler layer\n",
    "        # we use the output of the pooler which is of the size\n",
    "        # (batch_size, hidden_size)\n",
    "        # hidden size can be 768 or 1024 depending on\n",
    "        # if we are using bert base or large respectively\n",
    "        # in our case, it is 768\n",
    "        # note that this model is pretty simple\n",
    "        # you might want to use last hidden state\n",
    "        # or several hidden states\n",
    "        _, o2 = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # pass through dropout layer\n",
    "        bo = self.bert_drop(o2)\n",
    "        # pass through linear layer\n",
    "        output = self.out(bo)\n",
    "        # return output\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "crude-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "def loss_fn(outputs, targets):\n",
    "    \"\"\"\n",
    "    This function returns the loss.\n",
    "    :param outputs: output from the model (real numbers)\n",
    "    :param targets: input targets (binary)\n",
    "    \"\"\"\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    \"\"\"\n",
    "    This is the training function which trains for one epoch\n",
    "    :param data_loader: it is the torch dataloader object\n",
    "    :param model: torch model, bert in our case\n",
    "    :param optimizer: adam, sgd, etc\n",
    "    :param device: can be cpu or cuda\n",
    "    :param scheduler: learning rate scheduler\n",
    "    \"\"\"\n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "    # loop over all batches\n",
    "    for d in tqdm(data_loader):\n",
    "        # extract ids, token type ids and mask\n",
    "        # from current batch\n",
    "        # also extract targets\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets = d[\"targets\"]\n",
    "        # move everything to specified device\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        # zero-grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # pass through the model\n",
    "        outputs = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        # backward step the loss\n",
    "        loss.backward()\n",
    "        # step optimizer\n",
    "        optimizer.step()\n",
    "        # step scheduler\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alternative-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    this is the validation function that generates\n",
    "    predictions on validation data\n",
    "    :param data_loader: it is the torch dataloader object\n",
    "    :param model: torch model, bert in our case\n",
    "    :param device: can be cpu or cuda\n",
    "    :return: output and targets\n",
    "    \"\"\"\n",
    "    # put model in eval mode\n",
    "    model.eval()\n",
    "    # initialize empty lists for\n",
    "    # targets and outputs\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    # use the no_grad scope\n",
    "    # its very important else you might\n",
    "    # run out of gpu memory\n",
    "    with torch.no_grad():\n",
    "        # this part is same as training function\n",
    "        # except for the fact that there is no\n",
    "        # zero_grad of optimizer and there is no loss\n",
    "        # calculation or scheduler steps.\n",
    "        for d in tqdm(data_loader):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets = d[\"targets\"]\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            outputs = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "    # convert targets to cpu and extend the final list\n",
    "    targets = targets.cpu().detach()\n",
    "    fin_targets.extend(targets.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "analyzed-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # this function trains the model\n",
    "\n",
    "    # read the training file and fill NaN values with \"none\"\n",
    "    # you can also choose to drop NaN values in this\n",
    "    # specific dataset\n",
    "    dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
    "    # sentiment = 1 if its positive\n",
    "    # else sentiment = 0\n",
    "    dfx.sentiment = dfx.sentiment.apply(\n",
    "        lambda x: 1 if x == \"positive\" else 0\n",
    "    )\n",
    "    # we split the data into single training\n",
    "    # and validation fold\n",
    "    df_train, df_valid = model_selection.train_test_split(\n",
    "        dfx,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=dfx.sentiment.values\n",
    "    )\n",
    "    # reset index\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "    # for training dataset\n",
    "    train_dataset = BERTDataset(\n",
    "        review=df_train.review.values,\n",
    "        target=df_train.sentiment.values\n",
    "    )\n",
    "    # create training dataloader\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # for validation dataset\n",
    "    valid_dataset = BERTDataset(\n",
    "        review=df_valid.review.values,\n",
    "        target=df_valid.sentiment.values\n",
    "    )\n",
    "    # create validation data loader\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=VALID_BATCH_SIZE,\n",
    "        num_workers=1\n",
    "    )\n",
    "    # initialize the cuda device\n",
    "    # use cpu if you dont have GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    # load model and send it to the device\n",
    "    model = BERTBaseUncased()\n",
    "    model.to(device)\n",
    "    # create parameters we want to optimize\n",
    "    # we generally dont use any decay for bias\n",
    "    # and weight layers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if\n",
    "                not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if\n",
    "                any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    # calculate the number of training steps\n",
    "    # this is used by scheduler\n",
    "    num_train_steps = int(\n",
    "        len(df_train) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "    )\n",
    "    # AdamW optimizer\n",
    "    # AdamW is the most widely used optimizer\n",
    "    # for transformer based networks\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    # fetch a scheduler\n",
    "    # you can also try using reduce lr on plateau\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "    # if you have multiple GPUs\n",
    "    # model model to DataParallel\n",
    "    # to use multiple GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "    # start training the epochs\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_fn(\n",
    "        train_data_loader, model, optimizer, device, scheduler\n",
    "    )\n",
    "    outputs, targets = eval_fn(\n",
    "        valid_data_loader, model, device\n",
    "    )\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-anthropology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d30910ea64c49bc9251f892dcee011d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-demographic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
