{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mexican-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-basket",
   "metadata": {},
   "source": [
    "# Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caroline-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# this is the maximum number of tokens in the sentence\n",
    "MAX_LEN = 512\n",
    "# batch sizes is small because model is huge!\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "# let's train for a maximum of 10 epochs\n",
    "EPOCHS = 10\n",
    "# define path to BERT model files\n",
    "BERT_PATH = \"bert-base-uncased\"\n",
    "# this is where you want to save the model\n",
    "MODEL_PATH = \"model.bin\"\n",
    "# training file\n",
    "TRAINING_FILE = \"https://raw.githubusercontent.com/illyakaynov/masterclass-nlp/master/Case-IMBD_reviews/IMDB.csv\"\n",
    "# define the tokenizer\n",
    "# we use tokenizer and model\n",
    "# from huggingface's transformers\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    " BERT_PATH,\n",
    " do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protecting-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class BERTDataset:\n",
    "    def __init__(self, review, target):\n",
    "        \"\"\"\n",
    "        :param review: list or numpy array of strings\n",
    "        :param targets: list or numpy array which is binary\n",
    "        \"\"\"\n",
    "        self.review = review\n",
    "        self.target = target\n",
    "        \n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "        \n",
    "    def __len__(self):\n",
    "        # this returns the length of dataset\n",
    "        return len(self.review)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # for a given item index, return a dictionary\n",
    "        # of inputs\n",
    "        review = str(self.review[item])\n",
    "        review = \" \".join(review.split())\n",
    "        # encode_plus comes from hugginface's transformers\n",
    "        # and exists for all tokenizers they offer\n",
    "        # it can be used to convert a given string\n",
    "        # to ids, mask and token type ids which are\n",
    "        # needed for models like BERT\n",
    "        # here, review is a string\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,)\n",
    "        # ids are ids of tokens generated\n",
    "        # after tokenizing reviews\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        # mask is 1 where we have input\n",
    "        # and 0 where we have padding\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        # token type ids behave the same way as\n",
    "        # mask in this specific case\n",
    "        # in case of two sentences, this is 0\n",
    "        # for first sentence and 1 for second sentence\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        # now we return everything\n",
    "        # note that ids, mask and token_type_ids\n",
    "        # are all long datatypes and targets is float\n",
    "        return {\n",
    "            \"ids\": torch.tensor(\n",
    "                ids, dtype=torch.long\n",
    "            ),\n",
    "            \"mask\": torch.tensor(\n",
    "                mask, dtype=torch.long\n",
    "            ),\n",
    "            \"token_type_ids\": torch.tensor(\n",
    "                token_type_ids, dtype=torch.long\n",
    "            ),\n",
    "            \"targets\": torch.tensor(\n",
    "                self.target[item], dtype=torch.float\n",
    "            )\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "retired-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
    "        # add a dropout for regularization\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        # a simple linear layer for output\n",
    "        # yes, there is only one output\n",
    "        self.out = nn.Linear(768, 1)\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        # BERT in its default settings returns two outputs\n",
    "        # last hidden state and output of bert pooler layer\n",
    "        # we use the output of the pooler which is of the size\n",
    "        # (batch_size, hidden_size)\n",
    "        # hidden size can be 768 or 1024 depending on\n",
    "        # if we are using bert base or large respectively\n",
    "        # in our case, it is 768\n",
    "        # note that this model is pretty simple\n",
    "        # you might want to use last hidden state\n",
    "        # or several hidden states\n",
    "        _, o2 = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # pass through dropout layer\n",
    "        bo = self.bert_drop(o2)\n",
    "        # pass through linear layer\n",
    "        output = self.out(bo)\n",
    "        # return output\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "def loss_fn(outputs, targets):\n",
    "    \"\"\"\n",
    "    This function returns the loss.\n",
    "    :param outputs: output from the model (real numbers)\n",
    "    :param targets: input targets (binary)\n",
    "    \"\"\"\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    \"\"\"\n",
    "    This is the training function which trains for one epoch\n",
    "    :param data_loader: it is the torch dataloader object\n",
    "    :param model: torch model, bert in our case\n",
    "    :param optimizer: adam, sgd, etc\n",
    "    :param device: can be cpu or cuda\n",
    "    :param scheduler: learning rate scheduler\n",
    "    \"\"\"\n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "    # loop over all batches\n",
    "    for d in tqdm(data_loader):\n",
    "        # extract ids, token type ids and mask\n",
    "        # from current batch\n",
    "        # also extract targets\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets = d[\"targets\"]\n",
    "        # move everything to specified device\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        # zero-grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # pass through the model\n",
    "        outputs = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        # backward step the loss\n",
    "        loss.backward()\n",
    "        # step optimizer\n",
    "        optimizer.step()\n",
    "        # step scheduler\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "physical-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    this is the validation function that generates\n",
    "    predictions on validation data\n",
    "    :param data_loader: it is the torch dataloader object\n",
    "    :param model: torch model, bert in our case\n",
    "    :param device: can be cpu or cuda\n",
    "    :return: output and targets\n",
    "    \"\"\"\n",
    "    # put model in eval mode\n",
    "    model.eval()\n",
    "    # initialize empty lists for\n",
    "    # targets and outputs\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    # use the no_grad scope\n",
    "    # its very important else you might\n",
    "    # run out of gpu memory\n",
    "    with torch.no_grad():\n",
    "        # this part is same as training function\n",
    "        # except for the fact that there is no\n",
    "        # zero_grad of optimizer and there is no loss\n",
    "        # calculation or scheduler steps.\n",
    "        for d in tqdm(data_loader):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets = d[\"targets\"]\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            outputs = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "    # convert targets to cpu and extend the final list\n",
    "    targets = targets.cpu().detach()\n",
    "    fin_targets.extend(targets.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "completed-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # this function trains the model\n",
    "\n",
    "    # read the training file and fill NaN values with \"none\"\n",
    "    # you can also choose to drop NaN values in this\n",
    "    # specific dataset\n",
    "    dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
    "    # sentiment = 1 if its positive\n",
    "    # else sentiment = 0\n",
    "    dfx.sentiment = dfx.sentiment.apply(\n",
    "        lambda x: 1 if x == \"positive\" else 0\n",
    "    )\n",
    "    # we split the data into single training\n",
    "    # and validation fold\n",
    "    df_train, df_valid = model_selection.train_test_split(\n",
    "        dfx,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=dfx.sentiment.values\n",
    "    )\n",
    "    # reset index\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "    # for training dataset\n",
    "    train_dataset = BERTDataset(\n",
    "        review=df_train.review.values,\n",
    "        target=df_train.sentiment.values\n",
    "    )\n",
    "    # create training dataloader\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # for validation dataset\n",
    "    valid_dataset = BERTDataset(\n",
    "        review=df_valid.review.values,\n",
    "        target=df_valid.sentiment.values\n",
    "    )\n",
    "    # create validation data loader\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=VALID_BATCH_SIZE,\n",
    "        num_workers=1\n",
    "    )\n",
    "    # initialize the cuda device\n",
    "    # use cpu if you dont have GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    # load model and send it to the device\n",
    "    model = BERTBaseUncased()\n",
    "    model.to(device)\n",
    "    # create parameters we want to optimize\n",
    "    # we generally dont use any decay for bias\n",
    "    # and weight layers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if\n",
    "                not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if\n",
    "                any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    # calculate the number of training steps\n",
    "    # this is used by scheduler\n",
    "    num_train_steps = int(\n",
    "        len(df_train) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "    )\n",
    "    # AdamW optimizer\n",
    "    # AdamW is the most widely used optimizer\n",
    "    # for transformer based networks\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    # fetch a scheduler\n",
    "    # you can also try using reduce lr on plateau\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "    # if you have multiple GPUs\n",
    "    # model model to DataParallel\n",
    "    # to use multiple GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "    # start training the epochs\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_fn(\n",
    "        train_data_loader, model, optimizer, device, scheduler\n",
    "    )\n",
    "    outputs, targets = eval_fn(\n",
    "        valid_data_loader, model, device\n",
    "    )\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-complaint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d30910ea64c49bc9251f892dcee011d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
